{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5f865647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Marketing spends   Volume\n",
      "1 2017-01-01           176732  1942.21\n",
      "2 2017-02-01           180486  1749.93\n",
      "3 2017-03-01           180455  2399.42\n",
      "4 2017-04-01           185070  2126.85\n",
      "5 2017-05-01           195915   2242.5\n",
      "        Date Marketing spends   Volume\n",
      "1 2017-01-01           176732  1942.21\n",
      "2 2017-02-01           180486  1749.93\n",
      "3 2017-03-01           180455  2399.42\n",
      "4 2017-04-01           185070  2126.85\n",
      "5 2017-05-01           195915   2242.5\n",
      "        Date Marketing spends   Volume\n",
      "1 2017-01-01           176732  1942.21\n",
      "2 2017-02-01           180486  1749.93\n",
      "3 2017-03-01           180455  2399.42\n",
      "4 2017-04-01           185070  2126.85\n",
      "5 2017-05-01           195915   2242.5\n",
      "        Date Marketing spends   Volume\n",
      "1 2017-01-01           176732  1942.21\n",
      "2 2017-02-01           180486  1749.93\n",
      "3 2017-03-01           180455  2399.42\n",
      "4 2017-04-01           185070  2126.85\n",
      "5 2017-05-01           195915   2242.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:14:38 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Marketing spends   Volume\n",
      "1 2017-01-01           176732  1942.21\n",
      "2 2017-02-01           180486  1749.93\n",
      "3 2017-03-01           180455  2399.42\n",
      "4 2017-04-01           185070  2126.85\n",
      "5 2017-05-01           195915   2242.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:14:39 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24    1648.851857\n",
      "25    1624.350486\n",
      "26    1602.220215\n",
      "27    1577.718844\n",
      "28    1554.007840\n",
      "29    1529.506468\n",
      "30    1505.795464\n",
      "31    1481.294093\n",
      "32    1456.792721\n",
      "33    1433.081717\n",
      "34    1408.580346\n",
      "35    1384.869341\n",
      "Name: yhat, dtype: float64\n",
      "        Date Marketing spends   Volume\n",
      "1 2017-01-01           176732  1942.21\n",
      "2 2017-02-01           180486  1749.93\n",
      "3 2017-03-01           180455  2399.42\n",
      "4 2017-04-01           185070  2126.85\n",
      "5 2017-05-01           195915   2242.5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Dec  19 14:44:06 2019\n",
    "\n",
    "@author: Mohamed.Imran\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn import linear_model\n",
    "#from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#os.chdir(r\"C:\\Users\\moham\\OneDrive - University of Central Missouri\\Desktop\\Imran\\Training\\Inceptez\\Content\\Data-Science-Central-Online-master\\Data-Science-Central-Online-master\\Time Series\")\n",
    "#pwd = os.getcwd()\n",
    "\n",
    "data = pd.read_excel(\"TS_Data.xlsx\") #Read your decomposition file here\n",
    "data = data.tail(-1)\n",
    "#Regression model\n",
    "def Regression(data):\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data=data.sort_values([\"Date\"])\n",
    "    data[\"Quarter\"] = data[\"Date\"].dt.quarter\n",
    "    data[\"Year\"] = data[\"Date\"].dt.year\n",
    "    data[\"Month\"] = data[\"Date\"].dt.month\n",
    "    \n",
    "    Train=data[(data.Year>=2015)&(data.Year<2019)] #modify date according to your dataset; Train : 2017-2018\n",
    "    Test=data[(data.Year==2019)]  #modify date according to your dataset; Test : 2019\n",
    "    \n",
    "    \n",
    "    Train[\"SI_Y\"]=Train[\"Volume\"]/Train.groupby(\"Year\")[\"Volume\"].transform(np.mean)\n",
    "    Train[\"F_SI\"]=Train.groupby(\"Month\")[\"SI_Y\"].transform(np.mean)\n",
    "    Train[\"D_Seasonalised_trend\"] = Train[\"Volume\"]/Train[\"F_SI\"]    \n",
    "    Train[\"Level_index1\"]=np.mean(Train[(Train.Year==2018)&(Train.Quarter==1)][\"D_Seasonalised_trend\"])/np.mean(Train[(Train.Year==2017)&(Train.Quarter==4)][\"D_Seasonalised_trend\"])\n",
    "    \n",
    "    numer1=np.mean(Train[(Train.Year==2018)&(Train.Quarter==3)][\"D_Seasonalised_trend\"])/np.mean(Train[(Train.Year==2018)&(Train.Quarter==2)][\"D_Seasonalised_trend\"])\n",
    "    numer2=np.mean(Train[(Train.Year==2018)&(Train.Quarter==4)][\"D_Seasonalised_trend\"])/np.mean(Train[(Train.Year==2018)&(Train.Quarter==3)][\"D_Seasonalised_trend\"])\n",
    "    \n",
    "    \n",
    "    Train[\"Level_index2\"]=np.mean([numer1,numer2])\n",
    "    Train=Train.sort_values([\"Date\"])\n",
    "    Train.index=range(len(Train))\n",
    "    Train[\"ID\"]=range(1,(len(Train)+1))\n",
    "    \n",
    "    Train[\"Deleveled_series\"]=np.where(Train.Year==2017, Train[\"D_Seasonalised_trend\"]*Train[\"Level_index1\"],Train[\"D_Seasonalised_trend\"])\n",
    "    \n",
    "    lm = linear_model.LinearRegression()\n",
    "    X = np.array(Train[[\"ID\", \"Marketing spends\"]]) # In case of no extra variable in the dataset, remove the extra variable name from the list, then append the line with \".reshape(-1, 1)\"\n",
    "    Y = np.array(Train[\"Deleveled_series\"]).reshape(-1,1)\n",
    "    \n",
    "    model = lm.fit(X,Y)\n",
    "    \n",
    "    Test[\"ID\"]=range(len(Test))\n",
    "    Test[\"ID\"]=Test[\"ID\"]+max(Train[\"ID\"])\n",
    "    X_test=np.array(Test[[\"ID\", \"Marketing spends\"]]) # In case of no extra variable in the dataset, remove the extra variable name from the list, then append the line with \".reshape(-1, 1)\"\n",
    "    Y_test=model.predict(X_test)\n",
    "    \n",
    "    Pred1 = Y_test*Train.iloc[0][\"Level_index2\"]*np.array(Train.iloc[0:len(Y_test)][\"F_SI\"]).reshape(-1,1)\n",
    "    Test[\"Predictions\"]=Pred1\n",
    "    \n",
    "    return(Test['Predictions'])\n",
    "\n",
    "#Arima model\n",
    "def Arima(data): \n",
    "    X = data['Volume'].values\n",
    "    size = np.sum(data['Date']<='12/31/2018')\n",
    "    train, test = X[0:size], X[size:len(X)]\n",
    "    history = [x for x in train]\n",
    "    predictions = list()  \n",
    "        \n",
    "    for t in range(len(test)):\n",
    "    \tmodel = ARIMA(history, order=(1,1,0))\n",
    "    \tmodel_fit = model.fit()\n",
    "    \toutput = model_fit.forecast()\n",
    "    \tyhat = output[0]\n",
    "    \tpredictions.append(yhat)\n",
    "    \tobs = test[t]\n",
    "    \thistory.append(obs)\n",
    "    return predictions   \n",
    "\n",
    "#Holts-Winter model\n",
    "def Holts_winter(data):\n",
    "    inter_df = data[['Volume']]\n",
    "    size = np.sum(data['Date']<='12/31/2018')\n",
    "    train, test = inter_df.iloc[:size, 0], inter_df.iloc[size:, 0]\n",
    "    model = ExponentialSmoothing(np.array(train), seasonal='mul', seasonal_periods=12).fit()\n",
    "    pred = model.predict(start=test.index[0], end=test.index[-1])\n",
    "    return pd.DataFrame(pred, columns=['yhat'])\n",
    "\n",
    "#Fbprophet\n",
    "def Fbprophet(data):\n",
    "    size = np.sum(data['ds']<='12/31/2018')\n",
    "    inter_df = data.iloc[:size, :]\n",
    "    m = Prophet(weekly_seasonality=False, daily_seasonality=False)\n",
    "    m.fit(inter_df)\n",
    "    future = m.make_future_dataframe(periods=12, freq='M')\n",
    "    forecast = m.predict(future)\n",
    "    fcst = forecast['yhat'].tail(12)\n",
    "    return fcst\n",
    "\n",
    "#Simple Exponential Smoothing model\n",
    "def Ses(data):\n",
    "    inter_df = data[['Volume']]\n",
    "    size = np.sum(data['Date']<='12/31/2018')\n",
    "    train, test = inter_df.iloc[:size, 0], inter_df.iloc[size:, 0]\n",
    "    model = SimpleExpSmoothing(np.array(train)).fit()\n",
    "    pred = model.predict(start=test.index[0], end=test.index[-1])\n",
    "    return pd.DataFrame(pred, columns=['yhat'])\n",
    "\n",
    "\n",
    "\n",
    "def Regression_2lag(data):\n",
    "    data[\"Variable_1\"] = data[\"Marketing spends\"].shift(2)\n",
    "    data = data.loc[2:, :]\n",
    "    return Regression(data)\n",
    "\n",
    "\n",
    "required_cols = [col for col in data.columns if col not in ['Date', 'Marketing spends']]\n",
    "\n",
    "\n",
    "Result=pd.DataFrame()\n",
    "\n",
    "for model in [Regression, Arima, Holts_winter, Ses, Fbprophet, Regression_2lag]:\n",
    "    for i in required_cols:\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        to_func = data[[\"Date\", \"Marketing spends\", i]]\n",
    "        to_func.columns=[\"Date\",\"Marketing spends\", \"Volume\"]\n",
    "        print(to_func.head())\n",
    "        if model == Fbprophet:\n",
    "            to_func.columns=[\"ds\",\"Marketing spends\", \"y\"]\n",
    "            Result_inter = model(to_func[['ds', 'y']])\n",
    "            print(Result_inter)\n",
    "            Result_inter.name = model.__name__ + \"_\" +  i\n",
    "            Result_inter.index = range(len(Result_inter))\n",
    "            Result = pd.concat([Result, Result_inter], axis = 1)\n",
    "        elif model == Arima:\n",
    "            Result_inter = model(to_func)\n",
    "            Result_inter = pd.DataFrame(Result_inter, columns = [\"ARIMA_\" + i])\n",
    "            Result_inter.index=range(len(Result_inter))\n",
    "            Result = pd.concat([Result, Result_inter], axis = 1)\n",
    "        else:\n",
    "            Result_inter = model(to_func)\n",
    "            Result_inter.name = model.__name__ + \"_\" +  i\n",
    "            Result_inter.index=range(len(Result_inter))\n",
    "            Result = pd.concat([Result, Result_inter], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "Result.to_csv('Forecast.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f7859285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regression_Unnamed: 2</th>\n",
       "      <th>ARIMA_Unnamed: 2</th>\n",
       "      <th>yhat</th>\n",
       "      <th>yhat</th>\n",
       "      <th>Fbprophet_Unnamed: 2</th>\n",
       "      <th>Regression_2lag_Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1768.788713</td>\n",
       "      <td>1303.810064</td>\n",
       "      <td>1473.399434</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1648.851857</td>\n",
       "      <td>1655.679462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1644.867440</td>\n",
       "      <td>1794.298500</td>\n",
       "      <td>1714.389978</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1624.350486</td>\n",
       "      <td>1899.581115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1879.092523</td>\n",
       "      <td>1970.266340</td>\n",
       "      <td>1622.873362</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1602.220215</td>\n",
       "      <td>1786.388650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1768.116498</td>\n",
       "      <td>1909.750770</td>\n",
       "      <td>1718.502455</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1577.718844</td>\n",
       "      <td>1876.394032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1860.779308</td>\n",
       "      <td>1892.420707</td>\n",
       "      <td>1705.690392</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1554.007840</td>\n",
       "      <td>1798.965371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1783.719718</td>\n",
       "      <td>1955.382202</td>\n",
       "      <td>1625.238339</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1529.506468</td>\n",
       "      <td>1760.209644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1737.506669</td>\n",
       "      <td>1741.998443</td>\n",
       "      <td>1408.868509</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1505.795464</td>\n",
       "      <td>1490.943996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1467.594941</td>\n",
       "      <td>1860.362031</td>\n",
       "      <td>1775.133142</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1481.294093</td>\n",
       "      <td>1882.785277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1858.102359</td>\n",
       "      <td>1697.693296</td>\n",
       "      <td>2197.688795</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1456.792721</td>\n",
       "      <td>2284.272561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2261.536173</td>\n",
       "      <td>1921.679863</td>\n",
       "      <td>1829.755484</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1433.081717</td>\n",
       "      <td>1886.430392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1879.962712</td>\n",
       "      <td>2184.881903</td>\n",
       "      <td>1226.366496</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1408.580346</td>\n",
       "      <td>1228.841671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1232.671650</td>\n",
       "      <td>2003.681577</td>\n",
       "      <td>1555.008812</td>\n",
       "      <td>1739.648643</td>\n",
       "      <td>1384.869341</td>\n",
       "      <td>1927.482279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Regression_Unnamed: 2  ARIMA_Unnamed: 2         yhat         yhat  \\\n",
       "0             1768.788713       1303.810064  1473.399434  1739.648643   \n",
       "1             1644.867440       1794.298500  1714.389978  1739.648643   \n",
       "2             1879.092523       1970.266340  1622.873362  1739.648643   \n",
       "3             1768.116498       1909.750770  1718.502455  1739.648643   \n",
       "4             1860.779308       1892.420707  1705.690392  1739.648643   \n",
       "5             1783.719718       1955.382202  1625.238339  1739.648643   \n",
       "6             1737.506669       1741.998443  1408.868509  1739.648643   \n",
       "7             1467.594941       1860.362031  1775.133142  1739.648643   \n",
       "8             1858.102359       1697.693296  2197.688795  1739.648643   \n",
       "9             2261.536173       1921.679863  1829.755484  1739.648643   \n",
       "10            1879.962712       2184.881903  1226.366496  1739.648643   \n",
       "11            1232.671650       2003.681577  1555.008812  1739.648643   \n",
       "\n",
       "    Fbprophet_Unnamed: 2  Regression_2lag_Unnamed: 2  \n",
       "0            1648.851857                 1655.679462  \n",
       "1            1624.350486                 1899.581115  \n",
       "2            1602.220215                 1786.388650  \n",
       "3            1577.718844                 1876.394032  \n",
       "4            1554.007840                 1798.965371  \n",
       "5            1529.506468                 1760.209644  \n",
       "6            1505.795464                 1490.943996  \n",
       "7            1481.294093                 1882.785277  \n",
       "8            1456.792721                 2284.272561  \n",
       "9            1433.081717                 1886.430392  \n",
       "10           1408.580346                 1228.841671  \n",
       "11           1384.869341                 1927.482279  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236fe663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
